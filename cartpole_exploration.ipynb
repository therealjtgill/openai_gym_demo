{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What we're going to do:\n",
    "1. Examine OpenAI Gym with CartPole as an example environment\n",
    "2. Get hands-on experience with states (observations), actions, and rewards\n",
    "3. Do a little Q-Learning with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What are the three things that you need to do Q-Learning?\n",
    "1. States\n",
    "2. Actions\n",
    "3. Rewards\n",
    "\n",
    "We assume that when we're in some state, we can take an action, and get a reward back.\n",
    "\n",
    "#### State\n",
    "* Some representation of our relationship with the world\n",
    " * e.g. Pixels for Mario\n",
    " * e.g. position, velocity, angle, angular velocity for CartPole\n",
    "* There is usually a \"terminal state\" after which we can take no more actions\n",
    " \n",
    "#### Action\n",
    "* Some [noun] that we can perform that allows us to have an effect on our environment\n",
    " * e.g. It can allow us to move around in the environment\n",
    " \n",
    "#### Reward\n",
    "* Pretty self-explanatory...\n",
    "* Rewards can be positive, negative, or zero\n",
    "* We typically receive a reward every time we take an action\n",
    "\n",
    "* Actions and States can be continuous or discrete (but continuous things are usually converted into discrete things because they're easier to deal with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Examine OpenAI Gym with CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create an instance of the CartPole environment\n",
    "```\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reset the environment\n",
    "* Needs to be done before you use the environment\n",
    "```\n",
    "env.reset()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check out the actions we can take\n",
    "* OpenAI Gym wants us to be the best people we can be, so they don't label the actions that we can take\n",
    "* Not labeling actions forces us to make generic models\n",
    "```\n",
    "env.action_space\n",
    "```\n",
    "* How many actions are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Count the number of actions in the action space\n",
    "* There's no API call for this... which is a little suspicious\n",
    "```\n",
    "num_actions = 0\n",
    "while env.action_space.contains(num_actions):\n",
    "    num_actions += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check out the state (observation) space\n",
    "* Again, OpenAI Gym wants us to be the best people we can be, so we're not supposed to know what the numbers in the states correspond to\n",
    "```\n",
    "env.observation_space\n",
    "```\n",
    "* A Box just means that there are N dimensions, and each of those N dimensions has an upper and lower limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get the upper limit of the observation space\n",
    "```\n",
    "env.observation_space.high\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get the lower limit of the observation space\n",
    "```\n",
    "env.observation_space.low\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Get hands-on experience with states, actions, and rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Take a couple \"steps\" in the environment by taking actions\n",
    "* We know that actions are `Discrete(2)`, which means that we have to possible actions: 0 and 1\n",
    "* When we take an action in the environment (we take a \"step\"), the environment returns:\n",
    "```\n",
    "(new_state, reward, done, info)\n",
    "```\n",
    "* `new_state` is the state that we end up in by taking our specified action\n",
    "* `reward` is the number of points that we get by taking our action\n",
    "* `done` is a boolean indicator that we've reached a terminal state\n",
    "* `info` is... not really used for anything; it's usually a dictionary that's full of mysterious sadness\n",
    "\n",
    "```\n",
    "action = 0 # or 1\n",
    "new_state, reward, done, _ = env.step(action)\n",
    "print(new_state, reward)\n",
    "# Pick a new action if you want\n",
    "new_state, reward, done, _ = env.step(action)\n",
    "print(new_state, reward)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run an environment to completion\n",
    "* Basic idea is to take steps until the environment returns a `done` value of `True`\n",
    "* You can take random actions by sampling from the action space\n",
    "```\n",
    "new_state, reward, done, _ = env.step(env.action_space.sample())\n",
    "```\n",
    "* Just slap the line above into an appropriately terminated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Estimate the most likely states from this environment\n",
    "* This is totally a preprocessing step for the Q-Learning that we're going to do later\n",
    "* We want to append each of the elements of the new state vector to their own array\n",
    " * e.g. we have a list for each element of our state, and after each step we append the elements of the new states to the appropriate list\n",
    "* We're going to build a histogram of information about the most likely states\n",
    "```\n",
    "state_lists = []\n",
    "fake_state = env.reset()\n",
    "for i, fs in enumerate(fake_state):\n",
    "    state_lists.append([fs,])\n",
    "\n",
    "new_state, reward, done, _ = env.step(env.action_space.sample())\n",
    "for i, ns in enumerate(new_state):\n",
    "    state_lists[i].append(ns)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do this for a bunch of episodes\n",
    "* A single episode elapses after the environment returns a `done` of `True`\n",
    "* We want to do this a bunch of times, so\n",
    "```\n",
    "for i in range(LOTS_OF_EPISODES):\n",
    "    # probably something goes here\n",
    "    for j in range(500):\n",
    "        # appropriately terminating episode\n",
    "```\n",
    "* Don't forget to reset the environment after the end of each episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plot some histograms\n",
    "```\n",
    "for i in range(len(state_lists)):\n",
    "    plt.hist(state_lists[i])\n",
    "    plt.title(\"State feature \" + str(i))\n",
    "    plt.show()\n",
    "```\n",
    "* If you're feeling frisky, try using the bins parameter in the `plt.hist` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### If you want to render the environment, add an env.render()\n",
    "inside of one of the loops you've created. Or make a new loop. I'm a Jupyter notebook, not a cop.\n",
    "* You'll want to run the `env.render()` method outside of Jupyter, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
